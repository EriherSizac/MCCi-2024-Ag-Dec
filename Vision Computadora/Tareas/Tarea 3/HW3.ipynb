{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Salette Guadalupe Noemi Villalobos A01246619\n",
    "\n",
    "Erick Hernández Silva A01750170\n",
    "\n",
    "Israel Sánchez Miranda A01378705"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading imports and necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms\n",
    "from torchvision.models import resnet18\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score, KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning training and hyper parameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical-based filter selection and Principal Component Analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Digit Recognition using Machine Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we use a Machine Learning (ML) algorithm to recognize the digits of one member of the team. We created a dataset that consists in $10$ images each one is a page with $224$ digits each that go from $0$ to $9$. Then our dataset $D$ is compund of digits $d_i$ such that $d_i\\in\\{0,1,2,3,4,5,6,7,8,9\\}$. All digits are handwritten by one member of the team so the algorithm may be able to recognize the handwritting of such member. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train our model on the dataset we need to extract the features of each digit. The features we are going to extract are: \n",
    "1. The Hu moments. This features are scale and rotation invariant but can be sensitive to noise in the image.\n",
    "2. The Euler number. This feature will help to distinguish digit holes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions and considerations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we need to define a function to segment the $224$ digits from the source image. For this we define a function `segment_digits` that receives the path of the image and the output size of the segmented digit (by default we define an output image of $28\\times28$). We apply a threshold to the image using a binary Otsu algorithm, this due to the simple nature of the image where we have a white background (the page itself) and a black object (the digit). Then we use the `cv2.findCountours` function to detect and isolate the individual handwritten digits in the image. We feed to this function the previously thresholded image and ask the algorithm to return the outer contours (`RETR_EXTERNAL`) to avoid nested contours of each digit. We store the values of each corner using the `cv2.CHAIN_APPROX_SIMPLE` format to speed up the process without losing significant information about the shape.\n",
    "\n",
    "We then iterate over the contours found and calculate its bounding box using the `cv2.boundingRect` function and extract the digit from the thresholded image. We resize this image to the `output_size` and append it to the array that stores this images along with the coordinates of its bounding box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_digits(image_path, output_size=(28, 28)):\n",
    "  \"\"\"\n",
    "  Segments digits from a handwritten digit image.\n",
    "\n",
    "  Parameters:\n",
    "  - image_path: Path to the image file.\n",
    "  - output_size: Tuple indicating the size to resize the digit images.\n",
    "\n",
    "  Returns:\n",
    "  - A list of tuples (digit_image, bounding_box)\n",
    "  \"\"\"\n",
    "\n",
    "  # Load image in grayscale\n",
    "  img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "  # Threshold the image\n",
    "  _, thresh = cv2.threshold(img, 255, 255, cv2.THRESH_BINARY_INV | cv2.THRESH_OTSU)\n",
    "  # Find contours of the digits\n",
    "  contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "  digit_images = []\n",
    "  for cnt in contours:\n",
    "    x, y, w, h = cv2.boundingRect(cnt)\n",
    "    digit = thresh[y:y+h, x:x+w]\n",
    "\n",
    "    # Resize to standard size\n",
    "    digit_resized = cv2.resize(digit, output_size, interpolation=cv2.INTER_AREA)\n",
    "    digit_images.append((digit_resized, (x, y, w, h)))\n",
    "  return digit_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define the function `get_features` which extracts the Hu moments and Euler number of a given image. To get the Hu moments the function uses the `cv2.moments` function and then it uses the `cv2.HuMoments` function to finally flatten the result. We make a log scale transformation to reduce the dynamic range of the moments; in this way we improve numerical stability and enchance the discriminative power of the features. \n",
    "\n",
    "To calculate the Euler number we threshold the image using the `cv2.THRESH_BINARY` threshold. We then get this number calculating the connected components and substracting the number of holes each digit has (he substract one always to take into account the backgorund that is labeled with `0`).\n",
    "\n",
    "Finally, we store both the Hu moments and the Euler number in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(image):\n",
    "    \"\"\"\n",
    "    Calculates Hu moments and Euler number for a given image.\n",
    "\n",
    "    Parameters:\n",
    "    - image: The input image.\n",
    "\n",
    "    Returns:\n",
    "    - features: A list containing Hu moments and Euler number.\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate Hu Moments\n",
    "    moments = cv2.moments(image)\n",
    "    hu_moments = cv2.HuMoments(moments).flatten()\n",
    "\n",
    "    # Log scale transformation\n",
    "    hu_moments = -np.sign(hu_moments) * np.log10(np.abs(hu_moments) + 1e-10)\n",
    "\n",
    "    # Calculate Euler number\n",
    "    _, binary_image = cv2.threshold(image, 0, 1, cv2.THRESH_BINARY)\n",
    "    euler_number = cv2.connectedComponents(binary_image.astype(np.uint8))[0] - 1\n",
    "\n",
    "    features = list(hu_moments) + [euler_number]\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another function is defined; the `prepare_dataset`function takes the images from a folder with the following structure: \n",
    "- `folder/`\n",
    "  - `label_0.jpeg`\n",
    "  - `label_1.jpeg`\n",
    "  - `label_2.jpeg`\n",
    "  - `...`\n",
    "  - `label_n.jpeg\n",
    "\n",
    "  The function gets the label for each image and then applies the `segment_digits` to extract the digits of each image. Then, for each digit it gets the Hu moments and Euler number using the `get_features` function to finally store all these information into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(image_folder):\n",
    "    \"\"\"\n",
    "    Prepares the dataset from images in a folder.\n",
    "\n",
    "    Parameters:\n",
    "    - image_folder: Folder containing images (one image per class, class name must be the name of the image).\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame containing features and labels.\n",
    "    \"\"\"\n",
    "\n",
    "    data = []\n",
    "    labels = []\n",
    "    for filename in os.listdir(image_folder):\n",
    "        if filename.endswith('.png') or filename.endswith('.jpg') or filename.endswith('.jpeg'):\n",
    "            digit_label = os.path.splitext(filename)[0]\n",
    "            image_path = os.path.join(image_folder, filename)\n",
    "\n",
    "            digit_images = segment_digits(image_path)\n",
    "            for digit_image, _ in digit_images:\n",
    "                features = get_features(digit_image)\n",
    "                data.append(features)\n",
    "                labels.append(digit_label)\n",
    "\n",
    "    columns = [f'Hu{i+1}' for i in range(7)] + ['EulerNumber']\n",
    "    df = pd.DataFrame(data, columns=columns)\n",
    "    df['Digit'] = labels\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used a $k$-Nearest Neighbors ($k$ NN) algorithm to recognize the digits. We define as default $k=3$. In this function we use the `Digit` column as the target value and use the `KNeighborsClassifier` class from `sklearn` for the $k$ NN algorithm. We used a $k$-Fold cross-validation with $10$ folds using a shuffle in our dataset to better train the model and have a more general overview of how it performs. We print the cross-validation scores and the mean score for each fold. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_knn_model(df, k=3):\n",
    "    \"\"\"\n",
    "    Trains a k-NN classifier using 10-fold cross-validation.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame containing features and labels.\n",
    "    - k: Number of neighbors for k-NN.\n",
    "\n",
    "    Returns:\n",
    "    - Trained k-NN model.\n",
    "    - Cross-validation scores.\n",
    "    \"\"\"\n",
    "\n",
    "    X = df.drop('Digit', axis=1)\n",
    "    y = df['Digit']\n",
    "\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "    cv_scores = cross_val_score(knn, X, y, cv=kf)\n",
    "\n",
    "    knn.fit(X, y)\n",
    "    print(f'Cross-validation scores: {cv_scores}')\n",
    "    print(f'Mean CV score: {cv_scores.mean()}')\n",
    "    return knn, cv_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test our ML algorithm we use the `classify_id_image` function. This function receives the path to an image which is the one containing the ID of a student written by the same member of the team that wrote the digits originally in order for the ML algorithm to recognize them. It firstly segments each digit found on the image using the `segment_digits`function and then for each digit found it gets its features. The function then organizes each feature in a dataframe and makes a prediction with the mdoel using the `predict` function of the $k$ NN model. Finally we sort the predicted digits based on their $x$ coordinate to mantain order, finally, the function returns the digit predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_id_image(image_path, knn_model):\n",
    "    \"\"\"\n",
    "    Classifies digits from an image containing the student ID.\n",
    "\n",
    "    Parameters:\n",
    "    - image_path: Path to the student ID image.\n",
    "    - knn_model: Trained k-NN model.\n",
    "\n",
    "    Returns:\n",
    "    - List of predicted digits in order.\n",
    "    \"\"\"\n",
    "\n",
    "    digit_images = segment_digits(image_path)\n",
    "\n",
    "    digit_positions = []\n",
    "    digit_features = []\n",
    "    for digit_image, bbox in digit_images:\n",
    "        features = get_features(digit_image)\n",
    "        digit_features.append(features)\n",
    "        digit_positions.append(bbox)\n",
    "\n",
    "    # Predict digits\n",
    "    X_new = pd.DataFrame(digit_features, columns=[f'Hu{i+1}' for i in range(7)] + ['EulerNumber'])\n",
    "    predicted_digits = knn_model.predict(X_new)\n",
    "    \n",
    "    # Sort digits based on x-coordinate to maintain order\n",
    "    digits_with_positions = list(zip(predicted_digits, digit_positions))\n",
    "    digits_with_positions.sort(key=lambda x: x[1][0])  # Sort by x-coordinate\n",
    "    ordered_digits = [digit for digit, _ in digits_with_positions]\n",
    "    return ordered_digits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way we calculate the performance of the model is using the Levenshtein distance. This is, calculate how many deletions, insertions and substitutions we need to apply over a string $s_j$ to match an initial string $s_i$. This helps us to appreciate how similar is the predicted ID by the model with the original one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def levenshtein_distance(s1, s2):\n",
    "    \"\"\"\n",
    "    Calculates the Levenshtein distance between two strings.\n",
    "    \"\"\"\n",
    "    \n",
    "    if len(s1) < len(s2):\n",
    "      return levenshtein_distance(s2, s1)\n",
    "    \n",
    "    # Initialize previous row of distances\n",
    "    previous_row = list(range(len(s2) + 1))\n",
    "    for i, c1 in enumerate(s1):\n",
    "        current_row = [i + 1]\n",
    "        for j, c2 in enumerate(s2):\n",
    "            # Cost of deletions, insertions, and substitutions\n",
    "            deletions = previous_row[j + 1] + 1\n",
    "            insertions = current_row[j] + 1\n",
    "            substitutions = previous_row[j] + (c1 != c2)\n",
    "            current_row.append(min(deletions, insertions, substitutions))\n",
    "        previous_row = current_row\n",
    "    \n",
    "    return previous_row[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the algorihtm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tested the algorithm using thedataset defined at the [start of the section](#digit-recognition-using-machine-learning) using the `prepare_dataset` function over a folder with the structure defined at [Functions and considerations](#functions-and-considerations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset prepared.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hu1</th>\n",
       "      <th>Hu2</th>\n",
       "      <th>Hu3</th>\n",
       "      <th>Hu4</th>\n",
       "      <th>Hu5</th>\n",
       "      <th>Hu6</th>\n",
       "      <th>Hu7</th>\n",
       "      <th>EulerNumber</th>\n",
       "      <th>Digit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.139321</td>\n",
       "      <td>7.866246</td>\n",
       "      <td>9.607722</td>\n",
       "      <td>9.994978</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>-9.999999</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.470154</td>\n",
       "      <td>5.771757</td>\n",
       "      <td>9.556362</td>\n",
       "      <td>9.572124</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-9.999607</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.531530</td>\n",
       "      <td>6.019593</td>\n",
       "      <td>9.456941</td>\n",
       "      <td>9.087261</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.996963</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.438113</td>\n",
       "      <td>5.863366</td>\n",
       "      <td>9.253229</td>\n",
       "      <td>9.288303</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.998040</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.492358</td>\n",
       "      <td>5.891919</td>\n",
       "      <td>9.609553</td>\n",
       "      <td>9.323692</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>9.998321</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2669</th>\n",
       "      <td>2.680575</td>\n",
       "      <td>6.608726</td>\n",
       "      <td>8.188343</td>\n",
       "      <td>9.077046</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>-9.998576</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2670</th>\n",
       "      <td>2.618261</td>\n",
       "      <td>6.389477</td>\n",
       "      <td>7.991319</td>\n",
       "      <td>8.819356</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-9.998544</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2671</th>\n",
       "      <td>2.733390</td>\n",
       "      <td>7.603683</td>\n",
       "      <td>8.433619</td>\n",
       "      <td>9.446449</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>9.999985</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2672</th>\n",
       "      <td>2.676523</td>\n",
       "      <td>6.789756</td>\n",
       "      <td>8.161121</td>\n",
       "      <td>8.907812</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-9.999589</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2673</th>\n",
       "      <td>2.608245</td>\n",
       "      <td>6.291315</td>\n",
       "      <td>7.937914</td>\n",
       "      <td>8.898402</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>-9.998365</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2674 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Hu1       Hu2       Hu3       Hu4   Hu5       Hu6   Hu7  \\\n",
       "0     3.139321  7.866246  9.607722  9.994978 -10.0 -9.999999 -10.0   \n",
       "1     2.470154  5.771757  9.556362  9.572124  10.0 -9.999607 -10.0   \n",
       "2     2.531530  6.019593  9.456941  9.087261  10.0  9.996963  10.0   \n",
       "3     2.438113  5.863366  9.253229  9.288303  10.0  9.998040  10.0   \n",
       "4     2.492358  5.891919  9.609553  9.323692 -10.0  9.998321  10.0   \n",
       "...        ...       ...       ...       ...   ...       ...   ...   \n",
       "2669  2.680575  6.608726  8.188343  9.077046 -10.0 -9.998576  10.0   \n",
       "2670  2.618261  6.389477  7.991319  8.819356  10.0 -9.998544  10.0   \n",
       "2671  2.733390  7.603683  8.433619  9.446449 -10.0  9.999985  10.0   \n",
       "2672  2.676523  6.789756  8.161121  8.907812  10.0 -9.999589  10.0   \n",
       "2673  2.608245  6.291315  7.937914  8.898402 -10.0 -9.998365  10.0   \n",
       "\n",
       "      EulerNumber Digit  \n",
       "0               1     0  \n",
       "1               1     0  \n",
       "2               1     0  \n",
       "3               1     0  \n",
       "4               1     0  \n",
       "...           ...   ...  \n",
       "2669            1     9  \n",
       "2670            1     9  \n",
       "2671            1     9  \n",
       "2672            1     9  \n",
       "2673            1     9  \n",
       "\n",
       "[2674 rows x 9 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare the dataset\n",
    "dataset_folder = 'digits'\n",
    "df = prepare_dataset(dataset_folder)\n",
    "print('Dataset prepared.')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train our $k$ NN model using $k=2$. The model has a score of approximately $0.5$ which is not the best (basically a coin flip) but for educational purposes it works fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.57462687 0.55223881 0.45522388 0.54477612 0.52059925 0.4906367\n",
      " 0.50187266 0.52434457 0.59550562 0.53558052]\n",
      "Mean CV score: 0.5295404997484487\n",
      "k-NN model trained.\n"
     ]
    }
   ],
   "source": [
    "# Train the k-NN model\n",
    "k = 2\n",
    "knn_model, _ = train_knn_model(df, k=k)\n",
    "print('k-NN model trained.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then read the ID handwritten image to test the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Student ID: 1378730, Real ID is 1378705\n"
     ]
    }
   ],
   "source": [
    "# Classify digits from the student ID image\n",
    "id_image_path = 'test.jpeg'\n",
    "predicted_id = classify_id_image(id_image_path, knn_model)\n",
    "print(f'Predicted Student ID: {\"\".join(map(str, predicted_id))}, Real ID is 1378705')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the algorithm works fairly fine on predicting and extracting the ID from the image. We apply the Levenshtein distance function to see how different are both strings. We also apply a normalized error rate to have a better overview of the proportion of the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Levenshtein Distance (Error): 2\n",
      "Normalized Error Rate: 0.29\n"
     ]
    }
   ],
   "source": [
    "# Calculate Levenshtein distance\n",
    "predicted_id_str = \"\".join(map(str, predicted_id))\n",
    "actual_id_str = '1378705'\n",
    "error = levenshtein_distance(predicted_id_str, actual_id_str)\n",
    "print(f'Levenshtein Distance (Error): {error}')\n",
    "\n",
    "# Normalize Error Rate\n",
    "max_length = max(len(predicted_id_str), len(actual_id_str))\n",
    "error_rate = error / max_length\n",
    "print(f'Normalized Error Rate: {error_rate:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Levenshtein distance is of $2$, which means that at least two deletions, insertions or substitutions need to be applied for the predicted ID to be the same as the original ID. In this case we need two substitutions,change $s_6=3$ to $s_6=0$ and $s_7=0$ to $s_7=5$. The same way, we have an error rate of $0.29$ which suggest that almost a $30\\%$ of the predicted ID is wrong. A way we can improve this algorithm is to do an exploratory search of the $k$ NN hyper parameters (number of neighbors and similar configurations) to achieve an optimal solution. We can also use different threshold values when doing the digit segmentation to avoid extracting digits incorrectly and confuse the model with this input images. Finally, another thing we can do to improve the algorithm is to increase the number of digit samples to have a bigger dataset. For this we would also try to balance more each digit class because in some cases a digit can be written in more than one way. This was not taken into account when handwritting the dataset and may have result in an inter-class imbalance and thus, impacting greatly on the model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Digit Recognition using Deep Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to do digit recognition using Deep Learning models. In this section we use a ResNet CNN and train it over the same dataset as [past section](#digit-recognition-using-machine-learning) to predict a student ID."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we make a custom class `DigitDataset` that extends the `torch.utils.data.Dataset` class from PyTorch. This makes a custom dataset class to ease the compatibility between the data and the model. When we `__init__` the class we define an image directory and iterate over it to extract the labels and digits of an image using the `segment_digits()` function defined before. After that we append all this information to the `data` array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We redefine the `__getitem__` method to obtain the image from the `data` array, we convert the image to RGB format since we are treating with binary images and then return the image and its label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DigitDataset(Dataset):\n",
    "    def __init__(self, image_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "\n",
    "        # Load data and labels\n",
    "        for filename in os.listdir(image_dir):\n",
    "            if filename.endswith('.png') or filename.endswith('.jpg') or filename.endswith('.jpeg'):\n",
    "                label = int(os.path.splitext(filename)[0])\n",
    "                image_path = os.path.join(image_dir, filename)\n",
    "                digit_images = segment_digits(image_path)  # Assuming this function segments and returns (image, label)\n",
    "                \n",
    "                for digit_image, _ in digit_images:\n",
    "                    self.data.append(digit_image)\n",
    "                    self.labels.append(label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.data[idx]\n",
    "        \n",
    "        # Ensure the image has 3 channels for ResNet (H, W) -> (H, W, 3)\n",
    "        if image.ndim == 2:\n",
    "            image = np.stack([image] * 3, axis=-1)  # Convert grayscale to RGB\n",
    "\n",
    "        image = Image.fromarray(image)\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a `transforms` object to ensure each digit image has the correct dimensions (28x28 pixels), converting each image to a tensor and normalize it to a range the model will be trained on and to help the model recognize digits even with minor variations in digit position or orientation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we define the function `create_resnet_model` function to create a ResNet18 CNN with 10 classes as default (for digits $0$ to $9$) and add a final linear layer for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_resnet_model(num_classes=10):\n",
    "    model = resnet18(pretrained=True)  # Load ResNet18 with pretrained weights\n",
    "    model.fc = nn.Linear(model.fc.in_features, num_classes)  # Modify the final layer\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now prepare both the model and the dataset, we define our dataset with its data folder and the transform object. Then we define a `DataLoader` object that will load the data into the model with a batch size of $32$ and `shuffle=True` to better generalize the training. Then, we use the `create_resnet_model`function to define our model; we also define the loss criterion (in this case the cross entropy loss) and an optimizer (Adam optimizer) with a learning rate of $0.001$. We will train this model for $10$ epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Israel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Israel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to C:\\Users\\Israel/.cache\\torch\\hub\\checkpoints\\resnet18-f37072fd.pth\n",
      "100%|██████████| 44.7M/44.7M [00:04<00:00, 10.6MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "\n",
    "# Dataset and DataLoader\n",
    "train_dataset = DigitDataset('digits', transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Model, Loss, Optimizer\n",
    "model = create_resnet_model()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes training is too hard for a single CPU to handle, thus, if available, it is recommended to use a GPU (`cuda`) device to train the model. The line `model.to(device)` helps us to load the model into the device we will train it on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define a `train` function, we make a loop for the number of epochs, set the model into training mode and initialize the total loss. We train the model in batches of size $32$. First we get the inputs and its corresponding labels from the batch of the dataloader and move them to the same device we are training the model on. We make a zero-gradient forward pass of the model and compute the loss function of the outputs and its true labels. Then we make a backward pass and update the weights of the model using this criterion. Finally, we track the total loss for each epoch to obtain the average loss of the epoch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training function\n",
    "def train(model, dataloader, optimizer, criterion, device, num_epochs):\n",
    "    model.to(device)\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        # Wrap the dataloader in tqdm for a progress bar\n",
    "        for batch in tqdm(dataloader, desc=f\"Training Epoch {epoch + 1}\", unit=\"batch\"):\n",
    "            inputs, labels = batch\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()  # Clear gradients\n",
    "            \n",
    "            outputs = model(inputs)  # Forward pass\n",
    "            loss = criterion(outputs, labels)  # Compute loss\n",
    "            loss.backward()  # Backward pass\n",
    "            optimizer.step()  # Update weights\n",
    "            \n",
    "            total_loss += loss.item()  # Track total loss for epoch\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f\"Average Loss for Epoch {epoch + 1}: {avg_loss:.4f}\")\n",
    "        \n",
    "    print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we train the model using the functions and parameters defined before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 84/84 [08:08<00:00,  5.81s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss for Epoch 1: 0.4044\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 84/84 [08:19<00:00,  5.94s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss for Epoch 2: 0.3457\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 84/84 [08:42<00:00,  6.22s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss for Epoch 3: 0.3124\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 84/84 [10:00<00:00,  7.15s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss for Epoch 4: 0.3038\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 84/84 [09:10<00:00,  6.56s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss for Epoch 5: 0.3050\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6: 100%|██████████| 84/84 [10:58<00:00,  7.84s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss for Epoch 6: 0.2745\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7: 100%|██████████| 84/84 [10:26<00:00,  7.45s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss for Epoch 7: 0.2918\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8: 100%|██████████| 84/84 [08:22<00:00,  5.98s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss for Epoch 8: 0.2591\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9: 100%|██████████| 84/84 [08:00<00:00,  5.72s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss for Epoch 9: 0.2399\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10: 100%|██████████| 84/84 [07:51<00:00,  5.61s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss for Epoch 10: 0.2405\n",
      "Training complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train(model, train_loader, optimizer, criterion, device, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the model performs fairly well at the last epoch, having a relatively low loss of $0.2405$. Nevertheless, the model needs to be tested since this results may be due to overfit. It is also recommended to split the test dataset into a test and validation set to have a better overview of the performance. Also, cross-validation is another good tool for understanding the overall performance of a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we define a function that will segment the digits of the test image and for each digit found in the image we will predict it using our trained model. We set the model into evaluation mode, load the test image and make a prediction with this trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prediction function using your CNN model\n",
    "def predict_digits(model, image_path, transform, device):\n",
    "    # Ensure model is in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Segment digits from the image\n",
    "    segmented_digits = segment_digits(image_path)\n",
    "    \n",
    "    # Initialize list to hold predictions\n",
    "    predictions = []\n",
    "    \n",
    "    # Loop over each segmented digit\n",
    "    for digit_image, _ in segmented_digits:\n",
    "        # Convert the segmented digit to a PIL image\n",
    "        digit_pil = Image.fromarray(digit_image).convert('RGB')\n",
    "        \n",
    "        # Apply the same transformations as used during training\n",
    "        digit_tensor = transform(digit_pil).unsqueeze(0)  # Add batch dimension\n",
    "        \n",
    "        # Move tensor to the same device as the model\n",
    "        digit_tensor = digit_tensor.to(device)\n",
    "        \n",
    "        # Make prediction\n",
    "        with torch.no_grad():\n",
    "            outputs = model(digit_tensor)\n",
    "            _, pred = torch.max(outputs, dim=1)\n",
    "            predictions.append(str(pred.item()))  # Append the predicted digit as a string\n",
    "    \n",
    "    # Join predictions to form the student ID\n",
    "    student_id = ''.join(predictions)\n",
    "    return student_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make the same process as before, we load the image, predict the digits on it and use the Levenshtein distance to obtain the error of the prediciton."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted digit is: 5707831\n",
      "Levenshtein Distance (Error): 6\n",
      "Normalized Error Rate: 0.86\n"
     ]
    }
   ],
   "source": [
    "test_image_path = 'test.jpeg'\n",
    "predicted_digit = predict_digits(model, test_image_path, transform, device)\n",
    "print(f\"The predicted digit is: {predicted_digit}\")\n",
    "\n",
    "error = levenshtein_distance(predicted_digit, actual_id_str)\n",
    "print(f'Levenshtein Distance (Error): {error}')\n",
    "\n",
    "# Normalize Error Rate\n",
    "max_length = max(len(predicted_digit), len(actual_id_str))\n",
    "error_rate = error / max_length\n",
    "print(f'Normalized Error Rate: {error_rate:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Levenshtein distance is of $6$, which means that at least six deletions, insertions or substitutions need to be applied for the predicted ID to be the same as the original. The same way, we have an error rate of $0.86$ which suggest that almost an $86\\%$ of the predicted ID is wrong. This very bad results may be due to an overfitting in the model and the same problems we have with the ML model: low data representation, small dataset and the segmentation model needs fine tuning to correctly extract the numbers of each page. Additionally, we can opt to fine tune the parameters of the model like learning rate, chose a better loss function, add warm-up epochs, use a validation dataset, etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
