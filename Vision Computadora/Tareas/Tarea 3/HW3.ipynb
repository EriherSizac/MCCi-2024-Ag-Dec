{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Salette Guadalupe Noemi Villalobos A01246619\n",
    "\n",
    "Erick Hernández Silva A01750170\n",
    "\n",
    "Israel Sánchez Miranda A01378705"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading imports and necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms\n",
    "from torchvision.models import resnet18\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from ucimlrepo import fetch_ucirepo \n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Letter recognition with ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Overview\n",
    "The dataset used in this experiment is the Letter Recognition dataset from the UCI Machine Learning Repository. It contains 20,000 instances of character image features, where each instance represents a black-and-white pixel display of one of the 26 capital letters in the English alphabet. The objective is to classify each instance accurately as a particular letter based on 16 primitive numerical attributes. Each letter is represented with slight distortions across 20 different fonts, and the features capture properties like position, size, edge counts, and pixel correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'uci_id': 59, 'name': 'Letter Recognition', 'repository_url': 'https://archive.ics.uci.edu/dataset/59/letter+recognition', 'data_url': 'https://archive.ics.uci.edu/static/public/59/data.csv', 'abstract': 'Database of character image features; try to identify the letter', 'area': 'Computer Science', 'tasks': ['Classification'], 'characteristics': ['Multivariate'], 'num_instances': 20000, 'num_features': 16, 'feature_types': ['Integer'], 'demographics': [], 'target_col': ['lettr'], 'index_col': None, 'has_missing_values': 'no', 'missing_values_symbol': None, 'year_of_dataset_creation': 1991, 'last_updated': 'Thu Sep 28 2023', 'dataset_doi': '10.24432/C5ZP40', 'creators': ['David Slate'], 'intro_paper': None, 'additional_info': {'summary': 'The objective is to identify each of a large number of black-and-white rectangular pixel displays as one of the 26 capital letters in the English alphabet.  The character images were based on 20 different fonts and each letter within these 20 fonts was randomly distorted to produce a file of 20,000 unique stimuli.  Each stimulus was converted into 16 primitive numerical attributes (statistical moments and edge counts) which were then scaled to fit into a range of integer values from 0 through 15.  We typically train on the first 16000 items and then use the resulting model to predict the letter category for the remaining 4000.  See the article cited above for more details.', 'purpose': None, 'funded_by': None, 'instances_represent': None, 'recommended_data_splits': None, 'sensitive_data': None, 'preprocessing_description': None, 'variable_info': '\\t 1.\\tlettr\\tcapital letter\\t(26 values from A to Z)\\r\\n\\t 2.\\tx-box\\thorizontal position of box\\t(integer)\\r\\n\\t 3.\\ty-box\\tvertical position of box\\t(integer)\\r\\n\\t 4.\\twidth\\twidth of box\\t\\t\\t(integer)\\r\\n\\t 5.\\thigh \\theight of box\\t\\t\\t(integer)\\r\\n\\t 6.\\tonpix\\ttotal # on pixels\\t\\t(integer)\\r\\n\\t 7.\\tx-bar\\tmean x of on pixels in box\\t(integer)\\r\\n\\t 8.\\ty-bar\\tmean y of on pixels in box\\t(integer)\\r\\n\\t 9.\\tx2bar\\tmean x variance\\t\\t\\t(integer)\\r\\n\\t10.\\ty2bar\\tmean y variance\\t\\t\\t(integer)\\r\\n\\t11.\\txybar\\tmean x y correlation\\t\\t(integer)\\r\\n\\t12.\\tx2ybr\\tmean of x * x * y\\t\\t(integer)\\r\\n\\t13.\\txy2br\\tmean of x * y * y\\t\\t(integer)\\r\\n\\t14.\\tx-ege\\tmean edge count left to right\\t(integer)\\r\\n\\t15.\\txegvy\\tcorrelation of x-ege with y\\t(integer)\\r\\n\\t16.\\ty-ege\\tmean edge count bottom to top\\t(integer)\\r\\n\\t17.\\tyegvx\\tcorrelation of y-ege with x\\t(integer)', 'citation': None}}\n",
      "     name     role         type demographic                    description  \\\n",
      "0   lettr   Target  Categorical        None                 capital letter   \n",
      "1   x-box  Feature      Integer        None     horizontal position of box   \n",
      "2   y-box  Feature      Integer        None       vertical position of box   \n",
      "3   width  Feature      Integer        None                   width of box   \n",
      "4    high  Feature      Integer        None                  height of box   \n",
      "5   onpix  Feature      Integer        None              total # on pixels   \n",
      "6   x-bar  Feature      Integer        None     mean x of on pixels in box   \n",
      "7   y-bar  Feature      Integer        None     mean y of on pixels in box   \n",
      "8   x2bar  Feature      Integer        None                mean x variance   \n",
      "9   y2bar  Feature      Integer        None                mean y variance   \n",
      "10  xybar  Feature      Integer        None           mean x y correlation   \n",
      "11  x2ybr  Feature      Integer        None              mean of x * x * y   \n",
      "12  xy2br  Feature      Integer        None              mean of x * y * y   \n",
      "13  x-ege  Feature      Integer        None  mean edge count left to right   \n",
      "14  xegvy  Feature      Integer        None    correlation of x-ege with y   \n",
      "15  y-ege  Feature      Integer        None  mean edge count bottom to top   \n",
      "16  yegvx  Feature      Integer        None    correlation of y-ege with x   \n",
      "\n",
      "   units missing_values  \n",
      "0   None             no  \n",
      "1   None             no  \n",
      "2   None             no  \n",
      "3   None             no  \n",
      "4   None             no  \n",
      "5   None             no  \n",
      "6   None             no  \n",
      "7   None             no  \n",
      "8   None             no  \n",
      "9   None             no  \n",
      "10  None             no  \n",
      "11  None             no  \n",
      "12  None             no  \n",
      "13  None             no  \n",
      "14  None             no  \n",
      "15  None             no  \n",
      "16  None             no  \n"
     ]
    }
   ],
   "source": [
    "  \n",
    "# fetch dataset \n",
    "letter_recognition = fetch_ucirepo(id=59) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = letter_recognition.data.features \n",
    "y = letter_recognition.data.targets \n",
    "  \n",
    "# metadata \n",
    "print(letter_recognition.metadata) \n",
    "  \n",
    "# variable information \n",
    "print(letter_recognition.variables) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimental Setup\n",
    "**Data Preparation and Encoding**: The dataset was loaded and split into features X and target y. As the target variable (lettr) is categorical (representing letters A-Z), it was encoded to numerical values for compatibility with the machine learning models.\n",
    "\n",
    "**Data Splitting**: To evaluate model performance, we applied a 70-30 train-test split. This way, 70% of the data was used to train the models, while the remaining 30% served as a hold-out set to evaluate the model’s generalizability after hyperparameter tuning.\n",
    "\n",
    "**Cross-Validation**: The 10-fold cross-validation (CV) technique was used on the training set to get an unbiased estimate of each model's performance. In 10-fold CV, the training data is divided into 10 subsets, with each subset used once as a validation set, and the remaining 9 subsets used for training. This process helps mitigate overfitting and provides a robust assessment of the model’s performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'y' is a DataFrame with a single column, we select that column for dtype check\n",
    "if y.iloc[:, 0].dtype == 'object' or y.iloc[:, 0].nunique() > 1:\n",
    "    encoder = LabelEncoder()\n",
    "    y = encoder.fit_transform(y.iloc[:, 0])\n",
    "else:\n",
    "    y = y.iloc[:, 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train (70%) and hold-out (30%) sets\n",
    "X_train, X_holdout, y_train, y_holdout = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following three machine learning models were selected for this experiment:\n",
    "\n",
    "* **Random Forest**: A flexible, powerful ensemble method based on decision trees. It was chosen for its robustness and ability to capture complex patterns in the data.\n",
    "* **Support Vector Machine (SVM)**: Known for high performance in classification tasks, especially with complex decision boundaries.\n",
    "* **K-Nearest Neighbors (KNN)**: A straightforward yet effective method, chosen to evaluate how a simple distance-based classifier performs with this dataset.\n",
    "\n",
    "Each model was evaluated with 10-fold cross-validation to obtain an initial performance estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest 10-fold CV accuracy: 0.9539\n",
      "Support Vector Machine 10-fold CV accuracy: 0.9149\n",
      "K-Nearest Neighbors 10-fold CV accuracy: 0.9419\n"
     ]
    }
   ],
   "source": [
    "# Define the models to train and evaluate with cross-validation\n",
    "models = {\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42),\n",
    "    \"Support Vector Machine\": SVC(random_state=42),\n",
    "    \"K-Nearest Neighbors\": KNeighborsClassifier()\n",
    "}\n",
    "\n",
    "# Perform 10-fold cross-validation\n",
    "cv_results = {}\n",
    "for model_name, model in models.items():\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=10, scoring='accuracy')\n",
    "    cv_results[model_name] = scores.mean()\n",
    "    print(f\"{model_name} 10-fold CV accuracy: {scores.mean():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Hyperparameter Tuning with GridSearchCV\n",
    "We used GridSearchCV to find the best hyperparameters for each model. GridSearchCV performs an exhaustive search over a specified parameter grid, allowing us to identify the optimal settings for each model.\n",
    "\n",
    "The following hyperparameters were tuned for each model:\n",
    "\n",
    "* **Random Forest**: n_estimators (number of trees) and max_depth (maximum depth of each tree).\n",
    "* **SVM**: C (regularization parameter) and kernel (type of kernel function).\n",
    "* **KNN**: n_neighbors (number of neighbors) and weights (weight function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning hyperparameters for Random Forest...\n",
      "Best parameters for Random Forest: {'max_depth': 20, 'n_estimators': 150}\n",
      "Best cross-validated accuracy for Random Forest: 0.9554\n",
      "\n",
      "Tuning hyperparameters for Support Vector Machine...\n",
      "Best parameters for Support Vector Machine: {'C': 10, 'kernel': 'rbf'}\n",
      "Best cross-validated accuracy for Support Vector Machine: 0.9574\n",
      "\n",
      "Tuning hyperparameters for K-Nearest Neighbors...\n",
      "Best parameters for K-Nearest Neighbors: {'n_neighbors': 3, 'weights': 'distance'}\n",
      "Best cross-validated accuracy for K-Nearest Neighbors: 0.9510\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tune hyperparameters with GridSearchCV\n",
    "# Hyperparameter grids for each model\n",
    "param_grids = {\n",
    "    \"Random Forest\": {\n",
    "        \"n_estimators\": [50, 100, 150],\n",
    "        \"max_depth\": [None, 10, 20]\n",
    "    },\n",
    "    \"Support Vector Machine\": {\n",
    "        \"C\": [0.1, 1, 10],\n",
    "        \"kernel\": ['linear', 'rbf']\n",
    "    },\n",
    "    \"K-Nearest Neighbors\": {\n",
    "        \"n_neighbors\": [3, 5, 7],\n",
    "        \"weights\": ['uniform', 'distance']\n",
    "    }\n",
    "}\n",
    "\n",
    "# Dictionary to store the best models after tuning\n",
    "best_models = {}\n",
    "\n",
    "# Tuning each model using GridSearchCV with 10-fold cross-validation\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Tuning hyperparameters for {model_name}...\")\n",
    "    grid_search = GridSearchCV(model, param_grids[model_name], cv=10, scoring='accuracy')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    best_models[model_name] = grid_search.best_estimator_\n",
    "    print(f\"Best parameters for {model_name}: {grid_search.best_params_}\")\n",
    "    print(f\"Best cross-validated accuracy for {model_name}: {grid_search.best_score_:.4f}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation on Hold-Out Set\n",
    "After identifying the best hyperparameters, each tuned model was evaluated on the hold-out set (30% of the data that was not used in training). This allows us to see how each model generalizes to unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest hold-out set accuracy: 0.9635\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           A       0.99      0.98      0.99       237\n",
      "           B       0.89      0.97      0.93       230\n",
      "           C       0.96      0.95      0.96       221\n",
      "           D       0.92      0.98      0.95       242\n",
      "           E       0.97      0.96      0.96       230\n",
      "           F       0.96      0.95      0.95       232\n",
      "           G       0.95      0.94      0.95       232\n",
      "           H       0.92      0.89      0.91       220\n",
      "           I       0.97      0.92      0.95       226\n",
      "           J       0.96      0.96      0.96       224\n",
      "           K       0.95      0.94      0.94       222\n",
      "           L       0.99      0.97      0.98       228\n",
      "           M       0.99      0.98      0.99       238\n",
      "           N       0.99      0.94      0.97       235\n",
      "           O       0.97      0.96      0.96       226\n",
      "           P       0.96      0.97      0.97       241\n",
      "           Q       0.95      0.97      0.96       235\n",
      "           R       0.93      0.95      0.94       227\n",
      "           S       0.96      0.97      0.97       224\n",
      "           T       1.00      0.98      0.99       239\n",
      "           U       0.99      0.98      0.99       244\n",
      "           V       0.98      0.96      0.97       229\n",
      "           W       0.98      1.00      0.99       226\n",
      "           X       0.99      0.99      0.99       236\n",
      "           Y       0.97      0.99      0.98       236\n",
      "           Z       0.99      0.98      0.98       220\n",
      "\n",
      "    accuracy                           0.96      6000\n",
      "   macro avg       0.96      0.96      0.96      6000\n",
      "weighted avg       0.96      0.96      0.96      6000\n",
      "\n",
      "Support Vector Machine hold-out set accuracy: 0.9673\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           A       0.99      1.00      0.99       237\n",
      "           B       0.89      0.97      0.93       230\n",
      "           C       0.96      0.95      0.96       221\n",
      "           D       0.95      0.97      0.96       242\n",
      "           E       0.93      0.97      0.95       230\n",
      "           F       0.95      0.97      0.96       232\n",
      "           G       0.98      0.97      0.97       232\n",
      "           H       0.91      0.93      0.92       220\n",
      "           I       0.98      0.95      0.96       226\n",
      "           J       0.95      0.97      0.96       224\n",
      "           K       0.95      0.95      0.95       222\n",
      "           L       0.99      0.95      0.97       228\n",
      "           M       0.99      0.99      0.99       238\n",
      "           N       0.97      0.95      0.96       235\n",
      "           O       0.96      0.96      0.96       226\n",
      "           P       0.99      0.97      0.98       241\n",
      "           Q       0.97      1.00      0.98       235\n",
      "           R       0.94      0.89      0.91       227\n",
      "           S       0.99      0.98      0.99       224\n",
      "           T       1.00      0.97      0.98       239\n",
      "           U       0.99      0.99      0.99       244\n",
      "           V       1.00      0.96      0.98       229\n",
      "           W       0.99      0.99      0.99       226\n",
      "           X       0.98      0.96      0.97       236\n",
      "           Y       0.98      0.99      0.99       236\n",
      "           Z       0.99      0.99      0.99       220\n",
      "\n",
      "    accuracy                           0.97      6000\n",
      "   macro avg       0.97      0.97      0.97      6000\n",
      "weighted avg       0.97      0.97      0.97      6000\n",
      "\n",
      "K-Nearest Neighbors hold-out set accuracy: 0.9548\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           A       1.00      0.99      0.99       237\n",
      "           B       0.92      0.93      0.92       230\n",
      "           C       0.96      0.97      0.96       221\n",
      "           D       0.92      0.98      0.95       242\n",
      "           E       0.89      0.94      0.92       230\n",
      "           F       0.95      0.92      0.93       232\n",
      "           G       0.98      0.89      0.93       232\n",
      "           H       0.89      0.90      0.89       220\n",
      "           I       0.96      0.96      0.96       226\n",
      "           J       0.96      0.95      0.96       224\n",
      "           K       0.91      0.91      0.91       222\n",
      "           L       0.99      0.99      0.99       228\n",
      "           M       0.98      0.98      0.98       238\n",
      "           N       0.97      0.95      0.96       235\n",
      "           O       0.92      0.97      0.95       226\n",
      "           P       0.96      0.95      0.95       241\n",
      "           Q       0.97      0.96      0.97       235\n",
      "           R       0.92      0.92      0.92       227\n",
      "           S       0.98      0.96      0.97       224\n",
      "           T       0.97      0.98      0.97       239\n",
      "           U       0.98      0.98      0.98       244\n",
      "           V       0.97      0.95      0.96       229\n",
      "           W       0.97      0.99      0.98       226\n",
      "           X       0.95      0.96      0.96       236\n",
      "           Y       0.97      0.98      0.98       236\n",
      "           Z       0.98      0.99      0.98       220\n",
      "\n",
      "    accuracy                           0.95      6000\n",
      "   macro avg       0.96      0.95      0.95      6000\n",
      "weighted avg       0.96      0.95      0.95      6000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the best models on the hold-out set\n",
    "for model_name, best_model in best_models.items():\n",
    "    y_pred = best_model.predict(X_holdout)\n",
    "    accuracy = accuracy_score(y_holdout, y_pred)\n",
    "    print(f\"{model_name} hold-out set accuracy: {accuracy:.4f}\")\n",
    "    print(classification_report(y_holdout, y_pred, target_names=encoder.classes_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tested three machine learning models—Random Forest, Support Vector Machine (SVM), and K-Nearest Neighbors (KNN)—to classify letters based on image features. SVM achieved the highest accuracy on the hold-out set at 96.73%, closely followed by Random Forest at 96.35% and KNN at 95.48%. Hyperparameter tuning through cross-validation significantly enhanced each model's performance, confirming the importance of optimization. Overall, SVM demonstrated the best accuracy and generalizability, while Random Forest offers a strong alternative with the added benefit of feature interpretability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning training and hyper parameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical-based filter selection and Principal Component Analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Digit Recognition using Machine Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we use a Machine Learning (ML) algorithm to recognize the digits of one member of the team. We created a dataset that consists in $10$ images each one is a page with $224$ digits each that go from $0$ to $9$. Then our dataset $D$ is compund of digits $d_i$ such that $d_i\\in\\{0,1,2,3,4,5,6,7,8,9\\}$. All digits are handwritten by one member of the team so the algorithm may be able to recognize the handwritting of such member. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train our model on the dataset we need to extract the features of each digit. The features we are going to extract are: \n",
    "1. The Hu moments. This features are scale and rotation invariant but can be sensitive to noise in the image.\n",
    "2. The Euler number. This feature will help to distinguish digit holes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions and considerations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we need to define a function to segment the $224$ digits from the source image. For this we define a function `segment_digits` that receives the path of the image and the output size of the segmented digit (by default we define an output image of $28\\times28$). We apply a threshold to the image using a binary Otsu algorithm, this due to the simple nature of the image where we have a white background (the page itself) and a black object (the digit). Then we use the `cv2.findCountours` function to detect and isolate the individual handwritten digits in the image. We feed to this function the previously thresholded image and ask the algorithm to return the outer contours (`RETR_EXTERNAL`) to avoid nested contours of each digit. We store the values of each corner using the `cv2.CHAIN_APPROX_SIMPLE` format to speed up the process without losing significant information about the shape.\n",
    "\n",
    "We then iterate over the contours found and calculate its bounding box using the `cv2.boundingRect` function and extract the digit from the thresholded image. We resize this image to the `output_size` and append it to the array that stores this images along with the coordinates of its bounding box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_digits(image_path, output_size=(28, 28)):\n",
    "  \"\"\"\n",
    "  Segments digits from a handwritten digit image.\n",
    "\n",
    "  Parameters:\n",
    "  - image_path: Path to the image file.\n",
    "  - output_size: Tuple indicating the size to resize the digit images.\n",
    "\n",
    "  Returns:\n",
    "  - A list of tuples (digit_image, bounding_box)\n",
    "  \"\"\"\n",
    "\n",
    "  # Load image in grayscale\n",
    "  img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "  # Threshold the image\n",
    "  _, thresh = cv2.threshold(img, 255, 255, cv2.THRESH_BINARY_INV | cv2.THRESH_OTSU)\n",
    "  # Find contours of the digits\n",
    "  contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "  digit_images = []\n",
    "  for cnt in contours:\n",
    "    x, y, w, h = cv2.boundingRect(cnt)\n",
    "    digit = thresh[y:y+h, x:x+w]\n",
    "\n",
    "    # Resize to standard size\n",
    "    digit_resized = cv2.resize(digit, output_size, interpolation=cv2.INTER_AREA)\n",
    "    digit_images.append((digit_resized, (x, y, w, h)))\n",
    "  return digit_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define the function `get_features` which extracts the Hu moments and Euler number of a given image. To get the Hu moments the function uses the `cv2.moments` function and then it uses the `cv2.HuMoments` function to finally flatten the result. We make a log scale transformation to reduce the dynamic range of the moments; in this way we improve numerical stability and enchance the discriminative power of the features. \n",
    "\n",
    "To calculate the Euler number we threshold the image using the `cv2.THRESH_BINARY` threshold. We then get this number calculating the connected components and substracting the number of holes each digit has (he substract one always to take into account the backgorund that is labeled with `0`).\n",
    "\n",
    "Finally, we store both the Hu moments and the Euler number in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(image):\n",
    "    \"\"\"\n",
    "    Calculates Hu moments and Euler number for a given image.\n",
    "\n",
    "    Parameters:\n",
    "    - image: The input image.\n",
    "\n",
    "    Returns:\n",
    "    - features: A list containing Hu moments and Euler number.\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate Hu Moments\n",
    "    moments = cv2.moments(image)\n",
    "    hu_moments = cv2.HuMoments(moments).flatten()\n",
    "\n",
    "    # Log scale transformation\n",
    "    hu_moments = -np.sign(hu_moments) * np.log10(np.abs(hu_moments) + 1e-10)\n",
    "\n",
    "    # Calculate Euler number\n",
    "    _, binary_image = cv2.threshold(image, 0, 1, cv2.THRESH_BINARY)\n",
    "    euler_number = cv2.connectedComponents(binary_image.astype(np.uint8))[0] - 1\n",
    "\n",
    "    features = list(hu_moments) + [euler_number]\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another function is defined; the `prepare_dataset`function takes the images from a folder with the following structure: \n",
    "- `folder/`\n",
    "  - `label_0.jpeg`\n",
    "  - `label_1.jpeg`\n",
    "  - `label_2.jpeg`\n",
    "  - `...`\n",
    "  - `label_n.jpeg\n",
    "\n",
    "  The function gets the label for each image and then applies the `segment_digits` to extract the digits of each image. Then, for each digit it gets the Hu moments and Euler number using the `get_features` function to finally store all these information into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(image_folder):\n",
    "    \"\"\"\n",
    "    Prepares the dataset from images in a folder.\n",
    "\n",
    "    Parameters:\n",
    "    - image_folder: Folder containing images (one image per class, class name must be the name of the image).\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame containing features and labels.\n",
    "    \"\"\"\n",
    "\n",
    "    data = []\n",
    "    labels = []\n",
    "    for filename in os.listdir(image_folder):\n",
    "        if filename.endswith('.png') or filename.endswith('.jpg') or filename.endswith('.jpeg'):\n",
    "            digit_label = os.path.splitext(filename)[0]\n",
    "            image_path = os.path.join(image_folder, filename)\n",
    "\n",
    "            digit_images = segment_digits(image_path)\n",
    "            for digit_image, _ in digit_images:\n",
    "                features = get_features(digit_image)\n",
    "                data.append(features)\n",
    "                labels.append(digit_label)\n",
    "\n",
    "    columns = [f'Hu{i+1}' for i in range(7)] + ['EulerNumber']\n",
    "    df = pd.DataFrame(data, columns=columns)\n",
    "    df['Digit'] = labels\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In this experiment, we tested three machine learning models—Random Forest, Support Vector Machine (SVM), and K-Nearest Neighbors (KNN)—to classify letters based on image features. SVM achieved the highest accuracy on the hold-out set at 96.73%, closely followed by Random Forest at 96.35% and KNN at 95.48%. Hyperparameter tuning through cross-validation significantly enhanced each model's performance, confirming the importance of optimization. Overall, SVM demonstrated the best accuracy and generalizability, making it the most suitable model for deployment, while Random Forest offers a strong alternative with the added benefit of feature interpretability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used a $k$-Nearest Neighbors ($k$ NN) algorithm to recognize the digits. We define as default $k=3$. In this function we use the `Digit` column as the target value and use the `KNeighborsClassifier` class from `sklearn` for the $k$ NN algorithm. We used a $k$-Fold cross-validation with $10$ folds using a shuffle in our dataset to better train the model and have a more general overview of how it performs. We print the cross-validation scores and the mean score for each fold. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_knn_model(df, k=3):\n",
    "    \"\"\"\n",
    "    Trains a k-NN classifier using 10-fold cross-validation.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame containing features and labels.\n",
    "    - k: Number of neighbors for k-NN.\n",
    "\n",
    "    Returns:\n",
    "    - Trained k-NN model.\n",
    "    - Cross-validation scores.\n",
    "    \"\"\"\n",
    "\n",
    "    X = df.drop('Digit', axis=1)\n",
    "    y = df['Digit']\n",
    "\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "    cv_scores = cross_val_score(knn, X, y, cv=kf)\n",
    "\n",
    "    knn.fit(X, y)\n",
    "    print(f'Cross-validation scores: {cv_scores}')\n",
    "    print(f'Mean CV score: {cv_scores.mean()}')\n",
    "    return knn, cv_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test our ML algorithm we use the `classify_id_image` function. This function receives the path to an image which is the one containing the ID of a student written by the same member of the team that wrote the digits originally in order for the ML algorithm to recognize them. It firstly segments each digit found on the image using the `segment_digits`function and then for each digit found it gets its features. The function then organizes each feature in a dataframe and makes a prediction with the mdoel using the `predict` function of the $k$ NN model. Finally we sort the predicted digits based on their $x$ coordinate to mantain order, finally, the function returns the digit predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_id_image(image_path, knn_model):\n",
    "    \"\"\"\n",
    "    Classifies digits from an image containing the student ID.\n",
    "\n",
    "    Parameters:\n",
    "    - image_path: Path to the student ID image.\n",
    "    - knn_model: Trained k-NN model.\n",
    "\n",
    "    Returns:\n",
    "    - List of predicted digits in order.\n",
    "    \"\"\"\n",
    "\n",
    "    digit_images = segment_digits(image_path)\n",
    "\n",
    "    digit_positions = []\n",
    "    digit_features = []\n",
    "    for digit_image, bbox in digit_images:\n",
    "        features = get_features(digit_image)\n",
    "        digit_features.append(features)\n",
    "        digit_positions.append(bbox)\n",
    "\n",
    "    # Predict digits\n",
    "    X_new = pd.DataFrame(digit_features, columns=[f'Hu{i+1}' for i in range(7)] + ['EulerNumber'])\n",
    "    predicted_digits = knn_model.predict(X_new)\n",
    "    \n",
    "    # Sort digits based on x-coordinate to maintain order\n",
    "    digits_with_positions = list(zip(predicted_digits, digit_positions))\n",
    "    digits_with_positions.sort(key=lambda x: x[1][0])  # Sort by x-coordinate\n",
    "    ordered_digits = [digit for digit, _ in digits_with_positions]\n",
    "    return ordered_digits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way we calculate the performance of the model is using the Levenshtein distance. This is, calculate how many deletions, insertions and substitutions we need to apply over a string $s_j$ to match an initial string $s_i$. This helps us to appreciate how similar is the predicted ID by the model with the original one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def levenshtein_distance(s1, s2):\n",
    "    \"\"\"\n",
    "    Calculates the Levenshtein distance between two strings.\n",
    "    \"\"\"\n",
    "    \n",
    "    if len(s1) < len(s2):\n",
    "      return levenshtein_distance(s2, s1)\n",
    "    \n",
    "    # Initialize previous row of distances\n",
    "    previous_row = list(range(len(s2) + 1))\n",
    "    for i, c1 in enumerate(s1):\n",
    "        current_row = [i + 1]\n",
    "        for j, c2 in enumerate(s2):\n",
    "            # Cost of deletions, insertions, and substitutions\n",
    "            deletions = previous_row[j + 1] + 1\n",
    "            insertions = current_row[j] + 1\n",
    "            substitutions = previous_row[j] + (c1 != c2)\n",
    "            current_row.append(min(deletions, insertions, substitutions))\n",
    "        previous_row = current_row\n",
    "    \n",
    "    return previous_row[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the algorihtm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tested the algorithm using thedataset defined at the [start of the section](#digit-recognition-using-machine-learning) using the `prepare_dataset` function over a folder with the structure defined at [Functions and considerations](#functions-and-considerations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset prepared.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hu1</th>\n",
       "      <th>Hu2</th>\n",
       "      <th>Hu3</th>\n",
       "      <th>Hu4</th>\n",
       "      <th>Hu5</th>\n",
       "      <th>Hu6</th>\n",
       "      <th>Hu7</th>\n",
       "      <th>EulerNumber</th>\n",
       "      <th>Digit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.139321</td>\n",
       "      <td>7.866246</td>\n",
       "      <td>9.607722</td>\n",
       "      <td>9.994978</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>-9.999999</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.470154</td>\n",
       "      <td>5.771757</td>\n",
       "      <td>9.556362</td>\n",
       "      <td>9.572124</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-9.999607</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.531530</td>\n",
       "      <td>6.019593</td>\n",
       "      <td>9.456941</td>\n",
       "      <td>9.087261</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.996963</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.438113</td>\n",
       "      <td>5.863366</td>\n",
       "      <td>9.253229</td>\n",
       "      <td>9.288303</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.998040</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.492358</td>\n",
       "      <td>5.891919</td>\n",
       "      <td>9.609553</td>\n",
       "      <td>9.323692</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>9.998321</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2669</th>\n",
       "      <td>2.680575</td>\n",
       "      <td>6.608726</td>\n",
       "      <td>8.188343</td>\n",
       "      <td>9.077046</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>-9.998576</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2670</th>\n",
       "      <td>2.618261</td>\n",
       "      <td>6.389477</td>\n",
       "      <td>7.991319</td>\n",
       "      <td>8.819356</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-9.998544</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2671</th>\n",
       "      <td>2.733390</td>\n",
       "      <td>7.603683</td>\n",
       "      <td>8.433619</td>\n",
       "      <td>9.446449</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>9.999985</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2672</th>\n",
       "      <td>2.676523</td>\n",
       "      <td>6.789756</td>\n",
       "      <td>8.161121</td>\n",
       "      <td>8.907812</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-9.999589</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2673</th>\n",
       "      <td>2.608245</td>\n",
       "      <td>6.291315</td>\n",
       "      <td>7.937914</td>\n",
       "      <td>8.898402</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>-9.998365</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2674 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Hu1       Hu2       Hu3       Hu4   Hu5       Hu6   Hu7  \\\n",
       "0     3.139321  7.866246  9.607722  9.994978 -10.0 -9.999999 -10.0   \n",
       "1     2.470154  5.771757  9.556362  9.572124  10.0 -9.999607 -10.0   \n",
       "2     2.531530  6.019593  9.456941  9.087261  10.0  9.996963  10.0   \n",
       "3     2.438113  5.863366  9.253229  9.288303  10.0  9.998040  10.0   \n",
       "4     2.492358  5.891919  9.609553  9.323692 -10.0  9.998321  10.0   \n",
       "...        ...       ...       ...       ...   ...       ...   ...   \n",
       "2669  2.680575  6.608726  8.188343  9.077046 -10.0 -9.998576  10.0   \n",
       "2670  2.618261  6.389477  7.991319  8.819356  10.0 -9.998544  10.0   \n",
       "2671  2.733390  7.603683  8.433619  9.446449 -10.0  9.999985  10.0   \n",
       "2672  2.676523  6.789756  8.161121  8.907812  10.0 -9.999589  10.0   \n",
       "2673  2.608245  6.291315  7.937914  8.898402 -10.0 -9.998365  10.0   \n",
       "\n",
       "      EulerNumber Digit  \n",
       "0               1     0  \n",
       "1               1     0  \n",
       "2               1     0  \n",
       "3               1     0  \n",
       "4               1     0  \n",
       "...           ...   ...  \n",
       "2669            1     9  \n",
       "2670            1     9  \n",
       "2671            1     9  \n",
       "2672            1     9  \n",
       "2673            1     9  \n",
       "\n",
       "[2674 rows x 9 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare the dataset\n",
    "dataset_folder = 'digits'\n",
    "df = prepare_dataset(dataset_folder)\n",
    "print('Dataset prepared.')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train our $k$ NN model using $k=2$. The model has a score of approximately $0.5$ which is not the best (basically a coin flip) but for educational purposes it works fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.57462687 0.55223881 0.45522388 0.54477612 0.52059925 0.4906367\n",
      " 0.50187266 0.52434457 0.59550562 0.53558052]\n",
      "Mean CV score: 0.5295404997484487\n",
      "k-NN model trained.\n"
     ]
    }
   ],
   "source": [
    "# Train the k-NN model\n",
    "k = 2\n",
    "knn_model, _ = train_knn_model(df, k=k)\n",
    "print('k-NN model trained.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then read the ID handwritten image to test the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Student ID: 1378730, Real ID is 1378705\n"
     ]
    }
   ],
   "source": [
    "# Classify digits from the student ID image\n",
    "id_image_path = 'test.jpeg'\n",
    "predicted_id = classify_id_image(id_image_path, knn_model)\n",
    "print(f'Predicted Student ID: {\"\".join(map(str, predicted_id))}, Real ID is 1378705')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the algorithm works fairly fine on predicting and extracting the ID from the image. We apply the Levenshtein distance function to see how different are both strings. We also apply a normalized error rate to have a better overview of the proportion of the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Levenshtein Distance (Error): 2\n",
      "Normalized Error Rate: 0.29\n"
     ]
    }
   ],
   "source": [
    "# Calculate Levenshtein distance\n",
    "predicted_id_str = \"\".join(map(str, predicted_id))\n",
    "actual_id_str = '1378705'\n",
    "error = levenshtein_distance(predicted_id_str, actual_id_str)\n",
    "print(f'Levenshtein Distance (Error): {error}')\n",
    "\n",
    "# Normalize Error Rate\n",
    "max_length = max(len(predicted_id_str), len(actual_id_str))\n",
    "error_rate = error / max_length\n",
    "print(f'Normalized Error Rate: {error_rate:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Levenshtein distance is of $2$, which means that at least two deletions, insertions or substitutions need to be applied for the predicted ID to be the same as the original ID. In this case we need two substitutions,change $s_6=3$ to $s_6=0$ and $s_7=0$ to $s_7=5$. The same way, we have an error rate of $0.29$ which suggest that almost a $30\\%$ of the predicted ID is wrong. A way we can improve this algorithm is to do an exploratory search of the $k$ NN hyper parameters (number of neighbors and similar configurations) to achieve an optimal solution. We can also use different threshold values when doing the digit segmentation to avoid extracting digits incorrectly and confuse the model with this input images. Finally, another thing we can do to improve the algorithm is to increase the number of digit samples to have a bigger dataset. For this we would also try to balance more each digit class because in some cases a digit can be written in more than one way. This was not taken into account when handwritting the dataset and may have result in an inter-class imbalance and thus, impacting greatly on the model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Digit Recognition using Deep Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to do digit recognition using Deep Learning models. In this section we use a ResNet CNN and train it over the same dataset as [past section](#digit-recognition-using-machine-learning) to predict a student ID."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we make a custom class `DigitDataset` that extends the `torch.utils.data.Dataset` class from PyTorch. This makes a custom dataset class to ease the compatibility between the data and the model. When we `__init__` the class we define an image directory and iterate over it to extract the labels and digits of an image using the `segment_digits()` function defined before. After that we append all this information to the `data` array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We redefine the `__getitem__` method to obtain the image from the `data` array, we convert the image to RGB format since we are treating with binary images and then return the image and its label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DigitDataset(Dataset):\n",
    "    def __init__(self, image_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "\n",
    "        # Load data and labels\n",
    "        for filename in os.listdir(image_dir):\n",
    "            if filename.endswith('.png') or filename.endswith('.jpg') or filename.endswith('.jpeg'):\n",
    "                label = int(os.path.splitext(filename)[0])\n",
    "                image_path = os.path.join(image_dir, filename)\n",
    "                digit_images = segment_digits(image_path)  # Assuming this function segments and returns (image, label)\n",
    "                \n",
    "                for digit_image, _ in digit_images:\n",
    "                    self.data.append(digit_image)\n",
    "                    self.labels.append(label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.data[idx]\n",
    "        \n",
    "        # Ensure the image has 3 channels for ResNet (H, W) -> (H, W, 3)\n",
    "        if image.ndim == 2:\n",
    "            image = np.stack([image] * 3, axis=-1)  # Convert grayscale to RGB\n",
    "\n",
    "        image = Image.fromarray(image)\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a `transforms` object to ensure each digit image has the correct dimensions (28x28 pixels), converting each image to a tensor and normalize it to a range the model will be trained on and to help the model recognize digits even with minor variations in digit position or orientation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we define the function `create_resnet_model` function to create a ResNet18 CNN with 10 classes as default (for digits $0$ to $9$) and add a final linear layer for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_resnet_model(num_classes=10):\n",
    "    model = resnet18(pretrained=True)  # Load ResNet18 with pretrained weights\n",
    "    model.fc = nn.Linear(model.fc.in_features, num_classes)  # Modify the final layer\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now prepare both the model and the dataset, we define our dataset with its data folder and the transform object. Then we define a `DataLoader` object that will load the data into the model with a batch size of $32$ and `shuffle=True` to better generalize the training. Then, we use the `create_resnet_model`function to define our model; we also define the loss criterion (in this case the cross entropy loss) and an optimizer (Adam optimizer) with a learning rate of $0.001$. We will train this model for $10$ epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Israel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Israel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to C:\\Users\\Israel/.cache\\torch\\hub\\checkpoints\\resnet18-f37072fd.pth\n",
      "100%|██████████| 44.7M/44.7M [00:04<00:00, 10.6MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "\n",
    "# Dataset and DataLoader\n",
    "train_dataset = DigitDataset('digits', transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Model, Loss, Optimizer\n",
    "model = create_resnet_model()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes training is too hard for a single CPU to handle, thus, if available, it is recommended to use a GPU (`cuda`) device to train the model. The line `model.to(device)` helps us to load the model into the device we will train it on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define a `train` function, we make a loop for the number of epochs, set the model into training mode and initialize the total loss. We train the model in batches of size $32$. First we get the inputs and its corresponding labels from the batch of the dataloader and move them to the same device we are training the model on. We make a zero-gradient forward pass of the model and compute the loss function of the outputs and its true labels. Then we make a backward pass and update the weights of the model using this criterion. Finally, we track the total loss for each epoch to obtain the average loss of the epoch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training function\n",
    "def train(model, dataloader, optimizer, criterion, device, num_epochs):\n",
    "    model.to(device)\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        # Wrap the dataloader in tqdm for a progress bar\n",
    "        for batch in tqdm(dataloader, desc=f\"Training Epoch {epoch + 1}\", unit=\"batch\"):\n",
    "            inputs, labels = batch\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()  # Clear gradients\n",
    "            \n",
    "            outputs = model(inputs)  # Forward pass\n",
    "            loss = criterion(outputs, labels)  # Compute loss\n",
    "            loss.backward()  # Backward pass\n",
    "            optimizer.step()  # Update weights\n",
    "            \n",
    "            total_loss += loss.item()  # Track total loss for epoch\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f\"Average Loss for Epoch {epoch + 1}: {avg_loss:.4f}\")\n",
    "        \n",
    "    print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we train the model using the functions and parameters defined before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 84/84 [08:08<00:00,  5.81s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss for Epoch 1: 0.4044\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 84/84 [08:19<00:00,  5.94s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss for Epoch 2: 0.3457\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 84/84 [08:42<00:00,  6.22s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss for Epoch 3: 0.3124\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 84/84 [10:00<00:00,  7.15s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss for Epoch 4: 0.3038\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 84/84 [09:10<00:00,  6.56s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss for Epoch 5: 0.3050\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6: 100%|██████████| 84/84 [10:58<00:00,  7.84s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss for Epoch 6: 0.2745\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7: 100%|██████████| 84/84 [10:26<00:00,  7.45s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss for Epoch 7: 0.2918\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8: 100%|██████████| 84/84 [08:22<00:00,  5.98s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss for Epoch 8: 0.2591\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9: 100%|██████████| 84/84 [08:00<00:00,  5.72s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss for Epoch 9: 0.2399\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10: 100%|██████████| 84/84 [07:51<00:00,  5.61s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss for Epoch 10: 0.2405\n",
      "Training complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train(model, train_loader, optimizer, criterion, device, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the model performs fairly well at the last epoch, having a relatively low loss of $0.2405$. Nevertheless, the model needs to be tested since this results may be due to overfit. It is also recommended to split the test dataset into a test and validation set to have a better overview of the performance. Also, cross-validation is another good tool for understanding the overall performance of a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we define a function that will segment the digits of the test image and for each digit found in the image we will predict it using our trained model. We set the model into evaluation mode, load the test image and make a prediction with this trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prediction function using your CNN model\n",
    "def predict_digits(model, image_path, transform, device):\n",
    "    # Ensure model is in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Segment digits from the image\n",
    "    segmented_digits = segment_digits(image_path)\n",
    "    \n",
    "    # Initialize list to hold predictions\n",
    "    predictions = []\n",
    "    \n",
    "    # Loop over each segmented digit\n",
    "    for digit_image, _ in segmented_digits:\n",
    "        # Convert the segmented digit to a PIL image\n",
    "        digit_pil = Image.fromarray(digit_image).convert('RGB')\n",
    "        \n",
    "        # Apply the same transformations as used during training\n",
    "        digit_tensor = transform(digit_pil).unsqueeze(0)  # Add batch dimension\n",
    "        \n",
    "        # Move tensor to the same device as the model\n",
    "        digit_tensor = digit_tensor.to(device)\n",
    "        \n",
    "        # Make prediction\n",
    "        with torch.no_grad():\n",
    "            outputs = model(digit_tensor)\n",
    "            _, pred = torch.max(outputs, dim=1)\n",
    "            predictions.append(str(pred.item()))  # Append the predicted digit as a string\n",
    "    \n",
    "    # Join predictions to form the student ID\n",
    "    student_id = ''.join(predictions)\n",
    "    return student_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make the same process as before, we load the image, predict the digits on it and use the Levenshtein distance to obtain the error of the prediciton."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted digit is: 5707831\n",
      "Levenshtein Distance (Error): 6\n",
      "Normalized Error Rate: 0.86\n"
     ]
    }
   ],
   "source": [
    "test_image_path = 'test.jpeg'\n",
    "predicted_digit = predict_digits(model, test_image_path, transform, device)\n",
    "print(f\"The predicted digit is: {predicted_digit}\")\n",
    "\n",
    "error = levenshtein_distance(predicted_digit, actual_id_str)\n",
    "print(f'Levenshtein Distance (Error): {error}')\n",
    "\n",
    "# Normalize Error Rate\n",
    "max_length = max(len(predicted_digit), len(actual_id_str))\n",
    "error_rate = error / max_length\n",
    "print(f'Normalized Error Rate: {error_rate:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Levenshtein distance is of $6$, which means that at least six deletions, insertions or substitutions need to be applied for the predicted ID to be the same as the original. The same way, we have an error rate of $0.86$ which suggest that almost an $86\\%$ of the predicted ID is wrong. This very bad results may be due to an overfitting in the model and the same problems we have with the ML model: low data representation, small dataset and the segmentation model needs fine tuning to correctly extract the numbers of each page. Additionally, we can opt to fine tune the parameters of the model like learning rate, chose a better loss function, add warm-up epochs, use a validation dataset, etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "visioncompu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
